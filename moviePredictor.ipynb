{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nfrom sklearn.naive_bayes import MultinomialNB \nfrom sklearn.pipeline import Pipeline\nimport gensim\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport seaborn as sns # data visualization\nimport matplotlib.pyplot as plt # a collection of commands for making changes to plots\nimport csv, json, nltk, re # file handling module and natural language toolkit for text manipulation\nfrom nltk.corpus import stopwords # we need to use list of words ltr  \nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.linear_model import SGDClassifier\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"train_data = pd.read_csv(\"../input/train.csv\")\ntest_data = pd.read_csv(\"../input/test.csv\")\ntrain_data.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we cant see any movie that has a false value. False means it has no string text, but lets remove any movies\n# with no genres just in case\n\ntrain_data = train_data[~(train_data['genres'].str.len()==0)]\ntrain_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# maybe it is good idea to visualize the number of genres we have in our trining dataset by \n#counting their frequency\nallGenres = train_data['genres']\nallGenres = nltk.FreqDist(allGenres)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"allGenres","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can create a data frame that have two columns [Genre, Frequency]\ngenres_df = pd.DataFrame({'Genre':list(allGenres.keys()),'Frequency':list(allGenres.values())})\ngenres_df.head(6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets just visualize the most frequent 40 genres but you can visualize more\ngenres_df40 = genres_df.nlargest(columns='Frequency', n=40)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize = (15,10))\nax = sns.barplot(data =genres_df40, x ='Frequency', y ='Genre')\nax.set(xlabel = 'Frequency')\nax.set(ylabel = 'Genres')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# lets look at the synopsis just to see if we have noisy or text that wont contribute much to learn our model\nsynopsisData = train_data['synopsis']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split the text for later prediction \ndef splitString(textlist):\n\n    splitedtext = textlist.split(' ')\n    return splitedtext","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Ok, seems we can clean the synopsis by removing dots, commas or any underscore or backslah, chagnge letter\n#case for many reasons first stop words are in lower case,  etc\n\n#create a function so we can also use it more often\ndef clean_synopsis(synopsis_text):\n    # remove backslash\n    synopsis_text = re.sub(\"\\'\",\"\", synopsis_text)\n    # remove everything except string text\n    synopsis_text = re.sub(\"[^a-zA-Z]\",\" \", synopsis_text)\n    # remove white spaces in the synopsis\n    synopsis_text = ' '.join(synopsis_text.split())\n    # change all the text string to lower case\n    synopsis_text = synopsis_text.lower()\n    # return the clean synopsis\n    return synopsis_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can create a  similiar fcuntion as clean_synopsis and then use lambda\n# time to remove stop words because they not meaningful for our predictor. Thanks to nltk,\nstop_words = set(stopwords.words(\"english\"))\ndef stopword_removal(synopsis_text):\n    # use list comprehension\n    synopsis_text = [t for t in synopsis_text.split() if not t in stop_words]\n    # remove white spaces after stop words removed\n    synopsis_text = ' '.join(synopsis_text)\n    return synopsis_text","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def synopsis_analysis(movie_data):\n    \"\"\" First we remove backslah,\n    lowercase the letters, etc.  second\n    we remove stop words from the synopsis,\n     we use lambda function\n     input: a data frame with synopsis column\n     output: clean data\"\"\"\n    movie_data['synopsis_clean'] = movie_data['synopsis'].apply(lambda x: clean_synopsis(x))\n    movie_data['synopsis_clean'] = movie_data['synopsis_clean'].apply(lambda x: stopword_removal(x))\n\n    '''if 'genres' in movie_data.columns:\n\n        movie_data['new_genre'] = movie_data['genres'].apply(lambda x: splitString(x))'''\n\n    return movie_data\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data_new = synopsis_analysis(train_data)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we can construct our Pipeline to build a model from the training data and labels."},{"metadata":{"trusted":true},"cell_type":"code","source":"\n#tfidf_vectorizer = TfidfVectorizer()\nnb = Pipeline([('vect', CountVectorizer()),\n               ('tfidf', TfidfTransformer()),\n               ('clf', MultinomialNB()),\n              ])\nnb.fit(train_data_new['synopsis'], train_data_new['genres'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's use the movie synopsis to see how our learned model make the prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"prediction = nb.predict(test_data['synopsis'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Save our prediction to the output directory"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'movie_id': test_data['movie_id'], 'predicted_genres': prediction})\nfilename = 'Movie Genre Predictions NB 1.csv'\n\nsubmission.to_csv(filename, index=False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**Support Vector Machine **\n\nLets try to build a model using support vector machine. Perhaphs we get a better prediction"},{"metadata":{"trusted":true},"cell_type":"code","source":"sgd = Pipeline([('vect', CountVectorizer()),\n                ('tfidf', TfidfTransformer()),\n                ('clf', SGDClassifier(loss='hinge', penalty='l2',alpha=1e-3, random_state=42, max_iter=5, tol=None)),\n               ])\nsgd.fit(train_data_new['synopsis'], train_data_new['genres'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictionsgd = sgd.predict(test_data['synopsis'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame({'movie_id': test_data['movie_id'], 'predicted_genres': predictionsgd})\nfilename = 'Movie Genre Predictions SVM 1.csv'\n\nsubmission.to_csv(filename, index=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check the first 5 rows of our movies"},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Here we can binarize our target which is a movie genres if you want to use logistic regression"},{"metadata":{"trusted":true},"cell_type":"code","source":"# looks like we are mostly done with data preparation and exploration. Just need a few touches like encoding\n# the target variables with MultiLabelBinarizer(). Our target is the movie genres\nfrom sklearn.preprocessing import MultiLabelBinarizer\n\nmultilabel_binarizer = MultiLabelBinarizer()\nmultilabel_binarizer.fit(train_data_new['new_genre'])\n\n# transform target variable\ny = multilabel_binarizer.transform(train_data_new['new_genre'])\ntrainDataVectorized = tfidf_vectorizer.fit_transform(train_data_new['synopsis'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We need to use term frequency-inverse document frequency vectorization. The code below is data preparation for logistic regression algorithm"},{"metadata":{"trusted":true},"cell_type":"code","source":"# we can create a TF-IDF vectorization\ntrainDataVectorized = tfidf_vectorizer.fit_transform(trainData)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we can build our model\nfrom sklearn.linear_model import LogisticRegression\n\n# Binary Relevance\nfrom sklearn.multiclass import OneVsRestClassifier\n\n# Performance metric\nfrom sklearn.metrics import f1_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"logregressor = LogisticRegression()\nclf = OneVsRestClassifier(logregressor)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now we can fit the model on the training data\nclf.fit(trainDataVectorized,trainLabels)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# now we can make a prediction on the validation set\nyPredictor = clf.predict(valDataVectorized)\nyPredictor[0]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}